| Model | Architecture | Purpose | Multilinguality | Notable Features | Parameter Size | Paper |
|-------|--------------|---------|-----------------|-------------------|-----------------|-------|
| [FLAN](https://arxiv.org/abs/2109.01652) | Transformer Encoder-Decoder | Instruction-following, few-shot learning | Monolingual (English) | Few-shot learning, multi-task mixture-of-experts, retrieval augmentation | 137B | [Wei et al., 2022](https://arxiv.org/abs/2109.01652) |
| [T5](https://arxiv.org/abs/1910.10683) | Transformer Encoder-Decoder | Sequence-to-sequence tasks (translation, summarization) | Monolingual (English) | Text-to-text framework, unified model for various NLP tasks | 60M (Small), 220M (Base), 770M (Large), 3B (3B), 11B (11B) | [Raffel et al., 2020](https://arxiv.org/abs/1910.10683) |
| [BART](https://arxiv.org/abs/1910.13461) | Transformer Encoder-Decoder | Sequence-to-sequence tasks (translation, summarization) | Monolingual (English) | Combines bidirectional encoder and auto-regressive decoder | 139M (Base), 680M (Large) | [Lewis et al., 2020](https://arxiv.org/abs/1910.13461) |
| [mBART](https://arxiv.org/abs/2008.00401) | Transformer Encoder-Decoder | Machine translation, cross-lingual tasks | Multilingual (25+ languages) | Multilingual BART, cross-lingual transfer for generation tasks | 610M (Large) | [Liu et al., 2020](https://arxiv.org/abs/2008.00401) |
| [BERT](https://arxiv.org/abs/1810.04805) | Transformer Encoder | Transfer learning, NLP tasks (classification, QA, NER) | Monolingual (English) | Bidirectional encoder, masked language modeling pretraining | 110M (Base), 340M (Large) | [Devlin et al., 2019](https://arxiv.org/abs/1810.04805) |
| [RoBERTa](https://arxiv.org/abs/1907.11692) | Transformer Encoder | Transfer learning, NLP tasks (classification, QA, NER) | Monolingual (English) | Improved version of BERT, better pretraining | 125M (Base), 355M (Large) | [Liu et al., 2019](https://arxiv.org/abs/1907.11692) |
| [XLNet](https://arxiv.org/abs/1906.08237) | Transformer Encoder | Transfer learning, NLP tasks (classification, QA, NER) | Monolingual (English) | Permutation language modeling, better long-range dependencies | 110M (Base), 340M (Large) | [Yang et al., 2019](https://arxiv.org/abs/1906.08237) |
| [ALBERT](https://arxiv.org/abs/1909.11942) | Transformer Encoder | Transfer learning, NLP tasks (classification, QA, NER) | Monolingual (English) | Lite BERT, parameter-efficient, cross-layer parameter sharing | 12M (Base), 18M (Large) | [Lan et al., 2020](https://arxiv.org/abs/1909.11942) |
| [XLM-R](https://arxiv.org/abs/1911.02116) | Transformer Encoder | Transfer learning, cross-lingual NLP tasks | Multilingual (100+ languages) | Multilingual pretraining, zero-shot cross-lingual transfer | 270M (Base), 550M (Large) | [Conneau et al., 2020](https://arxiv.org/abs/1911.02116) |
| [GPT-2](https://openai.com/blog/better-language-models/) | Transformer Decoder | Text generation, open-ended language tasks | Monolingual (English) | Powerful autoregressive language model, zero-shot capabilities | 124M (Small), 355M (Medium), 774M (Large), 1.5B (XL) | [Radford et al., 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |
| [GPT-3](https://arxiv.org/abs/2005.14165) | Transformer Decoder | Text generation, open-ended language tasks | Monolingual (English) | Largest language model, few-shot learning, multimodal capabilities | 125M (Small), 1.3B (Medium), 3B (Large), 175B (XL) | [Brown et al., 2020](https://arxiv.org/abs/2005.14165) |
| [ELMo](https://arxiv.org/abs/1802.05365) | Bidirectional LSTM | Transfer learning, NLP tasks (classification, QA, NER) | Monolingual (English) | Deep contextualized word representations, character-based | 94M | [Peters et al., 2018](https://arxiv.org/abs/1802.05365) |
| [LaBSE](https://arxiv.org/abs/2007.01852) | Dual Encoder (BERT-based) | Multilingual sentence embeddings | Multilingual | High-quality sentence embeddings, BERT-based architecture | 110M (Base), 340M (Large) | [Feng et al., 2020](https://arxiv.org/abs/2007.01852) |
| [LASER](https://arxiv.org/abs/1812.10464) | Dual Encoder | Multilingual sentence embeddings | Multilingual | Lightweight, focused on generating sentence embeddings | 93M | [Artetxe and Schwenk, 2019](https://arxiv.org/abs/1812.10464) |
| [ULMFiT](https://arxiv.org/abs/1801.06146) | AWD-LSTM | Transfer learning, text classification | Monolingual (English) | Discriminative fine-tuning, slanted triangular learning rates | - | [Howard and Ruder, 2018](https://arxiv.org/abs/1801.06146) |

